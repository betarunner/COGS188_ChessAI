{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "/Users/primepi/anaconda3/envs/chess/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | seq  | Sequential | 2.0 M \n",
      "------------------------------------\n",
      "2.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 M     Total params\n",
      "3.924     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37164639\n",
      "b'CAAAAAAAAAAQAAAAAAAAAIEAAAAAAAAAJAAAAAAAAABCAAAAAAAAAADvABAAAAAAAAAAAAAAAAgAAAAAAAAAEAAAAAAAAACBAAAAAAAAACQAAAAAAAAAQgAAAAAAAP8AAAABEz8='\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/primepi/anaconda3/envs/chess/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/primepi/anaconda3/envs/chess/lib/python3.11/site-packages/lightning_fabric/utilities/data.py:63: UserWarning: Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading (when num_workers > 1), `__len__` could be inaccurate if each worker is not configured independently to avoid having duplicate data.\n",
      "  rank_zero_warn(\n",
      "/Users/primepi/anaconda3/envs/chess/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/10 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/primepi/anaconda3/envs/chess/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 10/10 [00:02<00:00,  3.79it/s, loss=4.57, v_num=nt-4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 10/10 [00:02<00:00,  3.75it/s, loss=4.57, v_num=nt-4]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import base64\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, IterableDataset, random_split\n",
    "import pytorch_lightning as pl\n",
    "from collections import OrderedDict\n",
    "from random import randrange\n",
    "from peewee import *\n",
    "\n",
    "# Connect to the SQLite database\n",
    "db = SqliteDatabase('2021-07-31-lichess-evaluations-37MM.db')\n",
    "\n",
    "# Define the Evaluations model\n",
    "class Evaluations(Model):\n",
    "    id = IntegerField()\n",
    "    fen = TextField()\n",
    "    binary = BlobField()\n",
    "    eval = FloatField()\n",
    "\n",
    "    class Meta:\n",
    "        database = db\n",
    "\n",
    "    def binary_base64(self):\n",
    "        return base64.b64encode(self.binary)\n",
    "\n",
    "# Connect to the database and print LABEL_COUNT\n",
    "db.connect()\n",
    "LABEL_COUNT = 37164639\n",
    "print(LABEL_COUNT)\n",
    "eval = Evaluations.get(Evaluations.id == 1)\n",
    "print(eval.binary_base64())\n",
    "\n",
    "# Define the EvaluationDataset\n",
    "class EvaluationDataset(IterableDataset):\n",
    "    def __init__(self, count, limit):\n",
    "        self.count = count\n",
    "        self.limit = limit\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        idx = randrange(self.limit)\n",
    "        return self[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.limit\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        eval = Evaluations.get(Evaluations.id == idx + 1)\n",
    "        bin = np.frombuffer(eval.binary, dtype=np.uint8)\n",
    "        bin = np.unpackbits(bin, axis=0).astype(np.single)\n",
    "        eval.eval = max(eval.eval, -15)\n",
    "        eval.eval = min(eval.eval, 15)\n",
    "        ev = np.array([eval.eval]).astype(np.single)\n",
    "        return {'binary': bin, 'eval': ev}\n",
    "\n",
    "# Limit the dataset to 20,058 entries\n",
    "dataset = EvaluationDataset(count=LABEL_COUNT, limit=20058)\n",
    "\n",
    "# Define the EvaluationModel\n",
    "class EvaluationModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3, batch_size=2048, layer_count=10):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        layers = []\n",
    "        for i in range(layer_count - 1):\n",
    "            layers.append((f\"linear-{i}\", nn.Linear(808, 808)))\n",
    "            layers.append((f\"relu-{i}\", nn.ReLU()))\n",
    "        layers.append((f\"linear-{layer_count - 1}\", nn.Linear(808, 1)))\n",
    "        self.seq = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['binary'], batch['eval']\n",
    "        y_hat = self(x)\n",
    "        loss = F.l1_loss(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "# Training configuration\n",
    "configs = [\n",
    "    {\"layer_count\": 4, \"batch_size\": 2048},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    version_name = f'{int(time.time())}-batch_size-{config[\"batch_size\"]}-layer_count-{config[\"layer_count\"]}'\n",
    "    logger = pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"chessml\", version=version_name)\n",
    "    trainer = pl.Trainer(devices=1, accelerator=\"mps\", precision=\"16\", max_epochs=1, logger=logger)\n",
    "    model = EvaluationModel(layer_count=config[\"layer_count\"], batch_size=config[\"batch_size\"], learning_rate=1e-3)\n",
    "    \n",
    "    trainer.fit(model)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model with no eval data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit None Automatic Mixed Precision (AMP)\n",
      "/Users/primepi/anaconda3/envs/chess/lib/python3.11/site-packages/torch/amp/grad_scaler.py:131: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type       | Params\n",
      "------------------------------------\n",
      "0 | seq  | Sequential | 1.9 M \n",
      "------------------------------------\n",
      "1.9 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.9 M     Total params\n",
      "3.862     Total estimated model params size (MB)\n",
      "/Users/primepi/anaconda3/envs/chess/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/primepi/anaconda3/envs/chess/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1609: PossibleUserWarning: The number of training batches (40) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/40 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/primepi/anaconda3/envs/chess/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.634, v_num=nt-4]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 40/40 [00:16<00:00,  2.38it/s, loss=0.634, v_num=nt-4]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import chess\n",
    "import chess.pgn\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Define the GamesDataset class\n",
    "class GamesDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.games_df = pd.read_csv(csv_file)\n",
    "        self.games_df['outcome'] = self.games_df['winner'].map({'white': 2, 'black': 0, 'draw': 1})  # Map results to integers\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.games_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.games_df.iloc[idx]\n",
    "        moves = row['moves']\n",
    "        outcome = row['outcome']\n",
    "        try:\n",
    "            fen = self.moves_to_fen(moves)\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting moves to FEN: {e}\")\n",
    "            return self.__getitem__((idx + 1) % len(self.games_df))  # Retry with the next item\n",
    "        board = chess.Board(fen)\n",
    "        binary_board = self.board_to_binary(board)\n",
    "        return {'binary': binary_board, 'outcome': torch.tensor(outcome, dtype=torch.long)}\n",
    "\n",
    "    def moves_to_fen(self, moves):\n",
    "        game = chess.pgn.Game()\n",
    "        node = game\n",
    "        board = chess.Board()\n",
    "        for move in moves.split():\n",
    "            try:\n",
    "                move_obj = board.parse_san(move)\n",
    "                board.push(move_obj)\n",
    "                node = node.add_main_variation(move_obj)\n",
    "            except ValueError as e:\n",
    "                raise Exception(f\"Invalid move: {move}, Error: {e}\")\n",
    "        return board.fen()\n",
    "\n",
    "    def board_to_binary(self, board):\n",
    "        # Convert board to a binary format suitable for NN input\n",
    "        binary = []\n",
    "        for square in chess.SQUARES:\n",
    "            piece = board.piece_at(square)\n",
    "            if piece:\n",
    "                binary.extend(self.piece_to_binary(piece))\n",
    "            else:\n",
    "                binary.extend([0] * 12)  # 12 channels for empty squares\n",
    "        if len(binary) != 768:\n",
    "            print(f\"Error: Binary board representation has incorrect length {len(binary)}\")\n",
    "        return torch.tensor(binary, dtype=torch.float)\n",
    "\n",
    "    def piece_to_binary(self, piece):\n",
    "        # 12 binary channels for each piece type and color\n",
    "        piece_map = {\n",
    "            'P': 0, 'N': 1, 'B': 2, 'R': 3, 'Q': 4, 'K': 5,\n",
    "            'p': 6, 'n': 7, 'b': 8, 'r': 9, 'q': 10, 'k': 11\n",
    "        }\n",
    "        binary = [0] * 12\n",
    "        binary[piece_map[piece.symbol()]] = 1\n",
    "        return binary\n",
    "\n",
    "# Define the OutcomeModel class\n",
    "class OutcomeModel(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3, batch_size=1024, layer_count=10):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        layers = []\n",
    "        layers.append(('flatten', nn.Flatten()))  # Add flatten layer to flatten the input\n",
    "        layers.append((f\"linear-0\", nn.Linear(768, 808)))\n",
    "        layers.append((f\"relu-0\", nn.ReLU()))\n",
    "        for i in range(1, layer_count - 1):\n",
    "            layers.append((f\"linear-{i}\", nn.Linear(808, 808)))\n",
    "            layers.append((f\"relu-{i}\", nn.ReLU()))\n",
    "        layers.append((f\"linear-{layer_count - 1}\", nn.Linear(808, 3)))  # 3 output classes for win, lose, draw\n",
    "        self.seq = nn.Sequential(OrderedDict(layers))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.seq(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch['binary'], batch['outcome']\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        dataset = GamesDataset(csv_file='games.csv')\n",
    "        return DataLoader(dataset, batch_size=self.batch_size, num_workers=0, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "# Training configuration\n",
    "configs = [\n",
    "    {\"layer_count\": 4, \"batch_size\": 512},\n",
    "    # {\"layer_count\": 6, \"batch_size\": 1024},\n",
    "]\n",
    "\n",
    "for config in configs:\n",
    "    version_name = f'{int(time.time())}-batch_size-{config[\"batch_size\"]}-layer_count-{config[\"layer_count\"]}'\n",
    "    logger = pl.loggers.TensorBoardLogger(\"lightning_logs\", name=\"chessml\", version=version_name)\n",
    "    trainer = pl.Trainer(devices=1, accelerator=\"mps\", precision=\"16\", max_epochs=1, logger=logger)\n",
    "    model = OutcomeModel(layer_count=config[\"layer_count\"], batch_size=config[\"batch_size\"], learning_rate=1e-3)\n",
    "    \n",
    "    # Uncomment below if you want to find optimal learning rate\n",
    "    # lr_finder = trainer.tuner.lr_find(model, min_lr=1e-6, max_lr=1e-3, num_training=25)\n",
    "    \n",
    "    # Plot the learning rate finder results\n",
    "    # fig = lr_finder.plot(suggest=True)\n",
    "    # fig.show()\n",
    "    \n",
    "    # Set the suggested learning rate\n",
    "    # new_lr = lr_finder.suggestion()\n",
    "    # model.learning_rate = new_lr\n",
    "    \n",
    "    trainer.fit(model)\n",
    "    break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
